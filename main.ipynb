{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Data Together into arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os, json\n",
    "import numpy as np\n",
    "from nltk.stem.snowball import EnglishStemmer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has been downloaded already, but it needs to be imported from the JSON objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Data\n",
    "skip = True\n",
    "title = []\n",
    "text = []\n",
    "url = []\n",
    "labels = []\n",
    "\n",
    "for x in os.walk(\"fakenewsnet_dataset/politifact/fake\"):\n",
    "    if skip:\n",
    "        skip = False\n",
    "        continue\n",
    "    try:\n",
    "        dir = x[0] + \"/news content.json\"\n",
    "        dir = dir.replace(\"\\\\\",\"/\")\n",
    "        o = json.load(open(dir))\n",
    "        if o[\"text\"] != \"\":\n",
    "            title.append(o[\"title\"])\n",
    "            text.append(o[\"text\"])\n",
    "            url.append(o[\"url\"])\n",
    "            labels.append(1)\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "\n",
    "skip = True\n",
    "for x in os.walk(\"fakenewsnet_dataset/politifact/real\"):\n",
    "    if skip:\n",
    "        skip = False\n",
    "        continue\n",
    "    try:\n",
    "        dir = x[0] + \"/news content.json\"\n",
    "        dir = dir.replace(\"\\\\\",\"/\")\n",
    "        o = json.load(open(dir))\n",
    "        if o[\"text\"] != \"\":\n",
    "            title.append(o[\"title\"])\n",
    "            text.append(o[\"text\"])\n",
    "            url.append(o[\"url\"])\n",
    "            labels.append(0)\n",
    "    except Exception:\n",
    "        continue"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data\n",
    "\n",
    "The TFIDF vectorizer handles a lot of the preprocessing, but it does not stem words, so that will be done using Natural Language Toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk import ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CURRENTLY NOT USED\n",
    "# def n_gram_tokenize(text, n):\n",
    "#     tokenized_text = text[:]\n",
    "#     stemmer = EnglishStemmer()\n",
    "#     ngram_text = []\n",
    "#     for i in range(len(text)):\n",
    "#         tokenized_text[i] = re.sub(r'[^A-Za-z ]+',' ',tokenized_text[i]).lower()\n",
    "#         tokenized_text[i] = [word for word in word_tokenize(tokenized_text[i]) if not word in stop_words]\n",
    "#         tokenized_text[i] = [stemmer.stem(word) for word in tokenized_text[i]]\n",
    "#         ngram_text.append(list(ngrams(tokenized_text[i],n)))\n",
    "#     return tokenized_text, ngram_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_text, ngram_text = n_gram_tokenize(text,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arr = np.array([title,url,text,tokenized_text,ngram_text,label],dtype=object).T\n",
    "# np.save(\"Pre-Processed Data\",arr)\n",
    "# dataset = pd.DataFrame(data=arr,columns=[\"Title\",\"URL\",\"Original Text\",\"Tokenized Text\",\"N-gram Text\",\"Label\"])\n",
    "# dataset['Label'] = dataset['Label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming using the english stemmer\n",
    "stemmer = EnglishStemmer()\n",
    "stemmed_text = text[:]\n",
    "stemmed_title = title[:]\n",
    "for i in range(len(text)):\n",
    "    stemmed_text[i] = ' '.join([stemmer.stem(word) for word in text[i].split(\" \")])\n",
    "    stemmed_title[i] = ' '.join([stemmer.stem(word) for word in title[i].split(\" \")])\n",
    "    stemmed_text[i] = stemmed_title[i] + stemmed_text[i]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Hyperparameters\n",
    "### (Pre-Processed data is loaded here)\n",
    "The hyperparameter to be tuned is the range of n-grams to use for the vectorizer. For example, a range of (1,3) means that the model is trained on uni-grams, bi-grams, and tri-grams. A range of (1,1) would mean the model only takes into account uni-grams, or just words by themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading data\n",
    "stemmed_text = np.array(stemmed_text)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(571,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(stemmed_text,labels,test_size=0.3,random_state=seed)\n",
    "\n",
    "#Sanity check\n",
    "X_train.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions below were used for cross-validation. Stochastic Gradient Descent with a linear SVM loss function was used to classify in cross-validation. Any classifier could be used since performance within a classifier would be appropriate to test the difference in accuracy with different n-gram usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def FeatureExtraction(X_train, X_test,m,n,max_df=0.5,min_df=10):\n",
    "\n",
    "    #Feature extraction\n",
    "    vectorizer = TfidfVectorizer(strip_accents='unicode',stop_words='english',lowercase=True,token_pattern=r'\\b[a-zA-Z]{3,}\\b',ngram_range=(m,n),max_df = max_df,min_df=min_df)\n",
    "    X_train_features = vectorizer.fit_transform(X_train)\n",
    "    \n",
    "    #Transforming validation set\n",
    "    X_test_features = vectorizer.transform(X_test)\n",
    "    \n",
    "    return X_train_features, X_test_features\n",
    "    \n",
    "def Classify(X_train_features,X_test_features,y_train,method='SGD'):\n",
    "\n",
    "    #Training classifier\n",
    "    if method == 'SGD':\n",
    "        classifier = SGDClassifier(loss='hinge',n_jobs=-1,random_state=seed)\n",
    "    elif method == 'Logistic':\n",
    "        classifier = LogisticRegression(n_jobs=-1,random_state=seed)\n",
    "    elif method == 'LinearSVM':\n",
    "        classifier = SVC(n_jobs=-1,random_state=seed,kernel='linear')\n",
    "    elif method == 'PolySVM':\n",
    "        classifier = SVC(n_jobs=-1,random_state=seed,kernel='poly')\n",
    "    elif method == 'Perceptron':\n",
    "        classifier = Perceptron(n_jobs=-1,random_state=seed)\n",
    "    else:\n",
    "        raise Exception(\"Unsupported method passed\")\n",
    "        \n",
    "    classifier.fit(X_train_features,y_train)\n",
    "\n",
    "    #Predicting training set\n",
    "    pred_train_labels = classifier.predict(X_train_features)\n",
    "\n",
    "    #Predicting test set\n",
    "    pred_test_labels = classifier.predict(X_test_features)\n",
    "\n",
    "    return pred_train_labels, pred_test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stratified cross-validation for tuning of n-gram range\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5,shuffle=True,random_state=seed)\n",
    "\n",
    "bestScore = 0\n",
    "bestM = 0\n",
    "bestN = 0\n",
    "\n",
    "for m in range(1,3):\n",
    "    for n in range(m,4):\n",
    "        avgTrain = 0\n",
    "        avgVal = 0\n",
    "        k = 1\n",
    "        for train_i, test_i in kfold.split(X_train,y_train):\n",
    "\n",
    "            #Getting feature matrices\n",
    "            X_train_features, X_test_features = FeatureExtraction(X_train[train_i],X_train[test_i],m,n)\n",
    "\n",
    "            #Classification\n",
    "            pred_train_labels, pred_test_labels = Classify(X_train_features,X_test_features,y_train[train_i])\n",
    "\n",
    "            #Validity metric\n",
    "            avgTrain += accuracy_score(y_train[train_i],pred_train_labels)\n",
    "            avgVal += accuracy_score(y_train[test_i],pred_test_labels)\n",
    "\n",
    "            k += 1\n",
    "\n",
    "        avgTrain = avgTrain / kfold.get_n_splits()\n",
    "        avgVal = avgVal / kfold.get_n_splits()\n",
    "\n",
    "        #Store best\n",
    "        if avgVal > bestScore:\n",
    "            bestScore = avgVal\n",
    "            bestM = m\n",
    "            bestN = n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best n-gram range is (1,1) with an average score of 0.865141\n"
     ]
    }
   ],
   "source": [
    "print(\"The best n-gram range is (%d,%d)\" % (bestM,bestN), \"with an average score of %f\" % bestScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 20\u001b[0m\n\u001b[0;32m     16\u001b[0m k \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[39mfor\u001b[39;00m train_i, test_i \u001b[39min\u001b[39;00m kfold\u001b[39m.\u001b[39msplit(X_train,y_train):\n\u001b[0;32m     18\u001b[0m \n\u001b[0;32m     19\u001b[0m     \u001b[39m#Getting feature matrices\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     X_train_features, X_test_features \u001b[39m=\u001b[39m FeatureExtraction(X_train[train_i],X_train[test_i],m,n,max_df\u001b[39m=\u001b[39;49mmax_df,min_df\u001b[39m=\u001b[39;49mmin_df)\n\u001b[0;32m     22\u001b[0m     \u001b[39m#Classification\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     pred_train_labels, pred_test_labels \u001b[39m=\u001b[39m Classify(X_train_features,X_test_features,y_train[train_i])\n",
      "Cell \u001b[1;32mIn[8], line 11\u001b[0m, in \u001b[0;36mFeatureExtraction\u001b[1;34m(X_train, X_test, m, n, max_df, min_df)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mFeatureExtraction\u001b[39m(X_train, X_test,m,n,max_df\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m,min_df\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m):\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m     \u001b[39m#Feature extraction\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     vectorizer \u001b[39m=\u001b[39m TfidfVectorizer(strip_accents\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39municode\u001b[39m\u001b[39m'\u001b[39m,stop_words\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m,lowercase\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,token_pattern\u001b[39m=\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mb[a-zA-Z]\u001b[39m\u001b[39m{\u001b[39m\u001b[39m3,}\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m,ngram_range\u001b[39m=\u001b[39m(m,n),max_df \u001b[39m=\u001b[39m max_df,min_df\u001b[39m=\u001b[39mmin_df)\n\u001b[1;32m---> 11\u001b[0m     X_train_features \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39;49mfit_transform(X_train)\n\u001b[0;32m     13\u001b[0m     \u001b[39m#Transforming validation set\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     X_test_features \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39mtransform(X_test)\n",
      "File \u001b[1;32mc:\\Users\\Ishaan\\anaconda3\\envs\\PR\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2121\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2114\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params()\n\u001b[0;32m   2115\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf \u001b[39m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2116\u001b[0m     norm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm,\n\u001b[0;32m   2117\u001b[0m     use_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_idf,\n\u001b[0;32m   2118\u001b[0m     smooth_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmooth_idf,\n\u001b[0;32m   2119\u001b[0m     sublinear_tf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublinear_tf,\n\u001b[0;32m   2120\u001b[0m )\n\u001b[1;32m-> 2121\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[0;32m   2122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[0;32m   2123\u001b[0m \u001b[39m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2124\u001b[0m \u001b[39m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ishaan\\anaconda3\\envs\\PR\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1377\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1369\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1370\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1371\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1372\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1373\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1374\u001b[0m             )\n\u001b[0;32m   1375\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1377\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1379\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1380\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Ishaan\\anaconda3\\envs\\PR\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1264\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1262\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[0;32m   1263\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[1;32m-> 1264\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1265\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1266\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32mc:\\Users\\Ishaan\\anaconda3\\envs\\PR\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:116\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mif\u001b[39;00m ngrams \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    115\u001b[0m     \u001b[39mif\u001b[39;00m stop_words \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 116\u001b[0m         doc \u001b[39m=\u001b[39m ngrams(doc, stop_words)\n\u001b[0;32m    117\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m         doc \u001b[39m=\u001b[39m ngrams(doc)\n",
      "File \u001b[1;32mc:\\Users\\Ishaan\\anaconda3\\envs\\PR\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:245\u001b[0m, in \u001b[0;36m_VectorizerMixin._word_ngrams\u001b[1;34m(self, tokens, stop_words)\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    240\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mnp.nan is an invalid document, expected byte or unicode string.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    241\u001b[0m         )\n\u001b[0;32m    243\u001b[0m     \u001b[39mreturn\u001b[39;00m doc\n\u001b[1;32m--> 245\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_word_ngrams\u001b[39m(\u001b[39mself\u001b[39m, tokens, stop_words\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    246\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Turn tokens into a sequence of n-grams after stop words filtering\"\"\"\u001b[39;00m\n\u001b[0;32m    247\u001b[0m     \u001b[39m# handle stop words\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5,shuffle=True,random_state=seed)\n",
    "\n",
    "m = bestM\n",
    "n = bestN\n",
    "bestScore = 0\n",
    "bestMinDf = 0\n",
    "bestMaxDf = 0\n",
    "\n",
    "for min_df in [10, 15, 20]:\n",
    "    for max_df in [0.4,0.5,0.6]:\n",
    "        avgTrain = 0\n",
    "        avgVal = 0\n",
    "        k = 1\n",
    "        for train_i, test_i in kfold.split(X_train,y_train):\n",
    "\n",
    "            #Getting feature matrices\n",
    "            X_train_features, X_test_features = FeatureExtraction(X_train[train_i],X_train[test_i],m,n,max_df=max_df,min_df=min_df)\n",
    "\n",
    "            #Classification\n",
    "            pred_train_labels, pred_test_labels = Classify(X_train_features,X_test_features,y_train[train_i])\n",
    "\n",
    "            avgTrain += accuracy_score(y_train[train_i],pred_train_labels)\n",
    "            avgVal += accuracy_score(y_train[test_i],pred_test_labels)\n",
    "\n",
    "            k += 1\n",
    "\n",
    "        avgTrain = avgTrain / kfold.get_n_splits()\n",
    "        avgVal = avgVal / kfold.get_n_splits()\n",
    "\n",
    "        if avgVal > bestScore:\n",
    "            bestScore = avgVal\n",
    "            bestMinDf = min_df\n",
    "            bestMaxDf = max_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best min and max df values are 10 and 0.40 respectively, with an average score of 0.851152\n"
     ]
    }
   ],
   "source": [
    "print(\"The best min and max df values are %d and %.2f respectively,\" % (bestMinDf,bestMaxDf), \"with an average score of %f\" % bestScore)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting feature matrices\n",
    "X_train_features, X_test_features = FeatureExtraction(X_train,X_test,m,n,max_df=bestMaxDf,min_df=bestMinDf)\n",
    "\n",
    "#Classification\n",
    "pred_train_labels, pred_test_labels = Classify(X_train_features,X_test_features,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9964973730297724\n",
      "0.8775510204081632\n",
      "[[321   0]\n",
      " [  2 248]]\n",
      "[[121  12]\n",
      " [ 18  94]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(accuracy_score(y_train,pred_train_labels))\n",
    "print(accuracy_score(y_test,pred_test_labels))\n",
    "print(confusion_matrix(y_train,pred_train_labels))\n",
    "print(confusion_matrix(y_test,pred_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(random_state=seed,kernel='linear')\n",
    "classifier.fit(X_train_features,y_train)\n",
    "\n",
    "train_labels = classifier.predict(X_train_features)\n",
    "\n",
    "test_labels = classifier.predict(X_test_features)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "print(accuracy_score(y_train,train_labels))\n",
    "print(accuracy_score(y_test,test_labels))\n",
    "print(confusion_matrix(y_train,train_labels))\n",
    "print(confusion_matrix(y_test,test_labels))\n",
    "# from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# classifier = LogisticRegression(random_state=seed)\n",
    "# classifier.fit(X_train_features,y_train)\n",
    "\n",
    "# train_labels = classifier.predict(X_train_features)\n",
    "\n",
    "# test_labels = classifier.predict(X_test_features)\n",
    "\n",
    "# print(accuracy_score(y_train,train_labels))\n",
    "# print(accuracy_score(y_test,test_labels))\n",
    "# print(confusion_matrix(y_train,train_labels))\n",
    "# print(confusion_matrix(y_test,test_labels))\n",
    "\n",
    "# classifier = Perceptron(random_state=seed)\n",
    "# classifier.fit(X_train_features,y_train)\n",
    "\n",
    "# train_labels = classifier.predict(X_train_features)\n",
    "\n",
    "# test_labels = classifier.predict(X_test_features)\n",
    "\n",
    "# print(accuracy_score(y_train,train_labels))\n",
    "# print(accuracy_score(y_test,test_labels))\n",
    "# print(confusion_matrix(y_train,train_labels))\n",
    "# print(confusion_matrix(y_test,test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
